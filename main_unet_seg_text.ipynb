{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # Clear CUDA cache\n",
    "# torch.cuda.empty_cache()\n",
    "# # Clear memory\n",
    "# gc.collect()\n",
    "\n",
    "# # Restart kernel\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.local/lib/python3.10/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from EDA import *\n",
    "from data_preprocessing import *\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "from seg_text_unet_model import *\n",
    "from loss import *\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import functools\n",
    "from text_utils import load_tokenizer, collate_fn_text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define output directory for saving test plots\n",
    "OUTPUT_DIR = \"visualizations/test_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cc/MMI_Unet_Lesion_Segmentation/data\n",
      "Data split completed: 2183 training, 273 validation, 273 testing.\n"
     ]
    }
   ],
   "source": [
    "#project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "project_dir = os.getcwd() \n",
    "data_path = os.path.join(project_dir, \"data\")\n",
    "print(data_path)\n",
    "\n",
    "# EDA - image sizes, top text descriptions, sample data\n",
    "#run_eda(data_path)\n",
    "\n",
    "# Split data into train, val and test\n",
    "split_data(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Added 2 special tokens.\n",
      "Tokenizer Vocabulary Size: 30522\n",
      "SOS token: '[SOS]', ID: 30522\n",
      "EOS token: '[EOS]', ID: 30523\n",
      "PAD token: '[PAD]', ID: 0\n",
      "PAD token ID: 0\n",
      "Vocabulary Size: 30522\n",
      "------------------------------\n",
      "SOS token ID: 30522\n",
      "EOS token ID: 30523\n",
      "PAD token ID: 0\n",
      "UNK token ID: 1\n",
      "Actual Vocab Size Used for Model: 30522\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = load_tokenizer()\n",
    "pad_id = tokenizer.pad_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"PAD token ID: {pad_id}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Optional: Define max text length if different from dataset default ---\n",
    "MAX_TEXT_LEN = 50 \n",
    "\n",
    "\n",
    "# In your notebook, after tokenizer = load_tokenizer()\n",
    "print(f\"SOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"UNK token ID: {tokenizer.unk_token_id}\") # Check UNK too\n",
    "print(f\"Actual Vocab Size Used for Model: {vocab_size}\") # The variable passed to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Number of training samples: 2183\n",
      "Number of validation samples: 273\n",
      "Number of testing samples: 273\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data - Resize and Augmentation\n",
    "train_transforms = A.Compose([\n",
    "    \n",
    "    A.RandomScale(scale_limit=0.1, p=0.1),  # 10% zoom\n",
    "    A.Resize(224, 224),\n",
    "    ToTensorV2() #convert to tensor and normalize\n",
    "])\n",
    "\n",
    "val_test_transforms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    ToTensorV2()   \n",
    "])\n",
    "\n",
    "# print(\"Creating datasets...\")\n",
    "# train_dataset = SegmentationDataset(data_path, \"train\", transform=train_transforms)\n",
    "# val_dataset = SegmentationDataset(data_path, \"val\", transform=val_test_transforms) \n",
    "# test_dataset = SegmentationDataset(data_path, \"test\",transform=val_test_transforms) \n",
    "\n",
    "# print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "# print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "# print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = SegmentationDataset(data_path, \"train\", tokenizer=tokenizer, transform=train_transforms, max_text_len=MAX_TEXT_LEN)\n",
    "val_dataset = SegmentationDataset(data_path, \"val\", tokenizer=tokenizer, transform=val_test_transforms, max_text_len=MAX_TEXT_LEN)\n",
    "test_dataset = SegmentationDataset(data_path, \"test\", tokenizer=tokenizer, transform=val_test_transforms, max_text_len=MAX_TEXT_LEN)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_with_tokenizer = functools.partial(collate_fn_text, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders...\n",
      "Data loaders created.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating data loaders...\")\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_with_tokenizer, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_tokenizer, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_with_tokenizer, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "print(\"Data loaders created.\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking one batch from train_loader...\n",
      "Batch loaded successfully!\n",
      "Images shape: torch.Size([8, 3, 224, 224])\n",
      "Masks shape: torch.Size([8, 1, 224, 224])\n",
      "Input IDs shape: torch.Size([8, 17])\n",
      "Target IDs shape: torch.Size([8, 17])\n",
      "Padding mask shape: torch.Size([8, 17])\n",
      "Input IDs example: tensor([30522,  8673,  3004,  2585,    16,  2159,  4291,  3685,    16,  3802,\n",
      "         2279,  2572,    18, 30523,     0])...\n",
      "Target IDs example: tensor([ 8673,  3004,  2585,    16,  2159,  4291,  3685,    16,  3802,  2279,\n",
      "         2572,    18, 30523,     0,     0])...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking one batch from train_loader...\")\n",
    "try:\n",
    "    images_batch, masks_batch, input_ids, target_ids, target_padding_mask = next(iter(train_loader))\n",
    "    print(\"Batch loaded successfully!\")\n",
    "    print(f\"Images shape: {images_batch.shape}\")       # Should be [B, C, H, W]\n",
    "    print(f\"Masks shape: {masks_batch.shape}\")         # Should be [B, 1, H, W]\n",
    "    print(f\"Input IDs shape: {input_ids.shape}\")       # Should be [B, T]\n",
    "    print(f\"Target IDs shape: {target_ids.shape}\")     # Should be [B, T]\n",
    "    print(f\"Padding mask shape: {target_padding_mask.shape}\") # Should be [B, T]\n",
    "    print(f\"Input IDs example: {input_ids[0, :15]}...\") # Print first few tokens\n",
    "    print(f\"Target IDs example: {target_ids[0, :15]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {e}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, masks, text_descriptions in train_loader:\n",
    "#     print(f\"Image type: {type(images)}\")\n",
    "#     print(f\"Image shape: {images.shape}\")\n",
    "#     print(f\"Image data type: {images.dtype}\") \n",
    "    \n",
    "#     print(f\"Text description type: {type(text_descriptions)}\")\n",
    "\n",
    "#     print(f\"Mask type: {type(masks)}\")\n",
    "#     print(f\"Mask shape: {masks.shape}\")\n",
    "#     print(f\"Mask data type: {masks.dtype}\")\n",
    "\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from seg_text_unet_model import JointSegTextUNet\n",
    "import torch\n",
    "\n",
    "#model = MMI_UNet(out_channels=1)\n",
    "\n",
    "#summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = torch.randn(1, 3, 224, 224)  \n",
    "# reports = [\n",
    "# \"Bilateral pulmonary infection, two infected areas, all left lung and middle right lung.\"\n",
    "# ]\n",
    "\n",
    "# # The model handles the text encoding internally\n",
    "# segmentation_maps = model(images, reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(segmentation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded seg_text_unet_model module.\n",
      "Inspecting ConvNeXtEncoder.__init__ signature:\n",
      "(self)\n",
      "\n",
      "File path for ConvNeXtEncoder:\n",
      "/home/cc/MMI_Unet_Lesion_Segmentation/seg_text_unet_model.py\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "    import importlib\n",
    "    import inspect\n",
    "    import seg_text_unet_model # Import the whole module first\n",
    "\n",
    "    # Force reload (in case caching is being stubborn)\n",
    "    importlib.reload(seg_text_unet_model)\n",
    "    print(\"Reloaded seg_text_unet_model module.\")\n",
    "\n",
    "    # Explicitly import the class *after* reload\n",
    "    from seg_text_unet_model import ConvNeXtEncoder\n",
    "\n",
    "    # Inspect the signature of the __init__ method Python is actually seeing\n",
    "    print(\"Inspecting ConvNeXtEncoder.__init__ signature:\")\n",
    "    try:\n",
    "        print(inspect.signature(ConvNeXtEncoder.__init__))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not inspect signature: {e}\")\n",
    "\n",
    "    # Print the file Python loaded the class from\n",
    "    print(\"\\nFile path for ConvNeXtEncoder:\")\n",
    "    try:\n",
    "        print(inspect.getfile(ConvNeXtEncoder))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get file path: {e}\")\n",
    "\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Model initialized successfully.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from loss import *\n",
    "from train import *\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "EMBED_DIM = 768 # Matching ConvNeXt F4, RATCHET used 512\n",
    "NHEAD = 8\n",
    "NUM_DECODER_LAYERS = 6\n",
    "DIM_FEEDFORWARD = 3072 # Typically 4*EMBED_DIM, RATCHET used 2048\n",
    "MAX_TEXT_SEQ_LEN = 50  #based on dataset\n",
    "DROPOUT = 0.1 # RATCHET used 0.2\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model = JointSegTextUNet(\n",
    "    seg_out_channels=1, \n",
    "    vocab_size=vocab_size+2, \n",
    "    embed_dim=EMBED_DIM,\n",
    "    nhead=NHEAD,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    max_text_seq_len=MAX_TEXT_SEQ_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    pad_token_id=pad_id \n",
    ").to(device) \n",
    "\n",
    "print(\"Model initialized successfully.\")\n",
    "# Print model summary or parameter count\n",
    "# print(model)\n",
    "# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"Total trainable parameters: {total_params:,}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image outputs will be saved to: visualizations/test_outputs\n",
      "Training and testing logs will be saved to: training_log_segtext_cloud.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import contextlib\n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "\n",
    "# --- Configuration ---\n",
    "LOG_FILE_PATH = \"training_log_segtext_cloud.txt\"\n",
    "TEST_OUTPUT_DIR = \"visualizations/test_outputs\"\n",
    "\n",
    "os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Test image outputs will be saved to: {TEST_OUTPUT_DIR}\")\n",
    "print(f\"Training and testing logs will be saved to: {LOG_FILE_PATH}\") \n",
    "\n",
    "\n",
    "# --- Context Manager for Logging ---\n",
    "@contextlib.contextmanager\n",
    "def log_stdout_to_file(filename, mode='w'): \n",
    "    original_stdout = sys.stdout\n",
    "    try:\n",
    "        with open(filename, mode) as log_file: # Use the mode argument\n",
    "            sys.stdout = log_file\n",
    "            yield # Executes the code within the 'with' block\n",
    "    finally:\n",
    "        sys.stdout = original_stdout # Restores normal printing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.local/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Train the model and save output to log file\n",
    "print(\"Starting Training...\") # Prints to notebook\n",
    "with log_stdout_to_file(LOG_FILE_PATH): # Use 'w' to overwrite/start log\n",
    "    print(\"--- Training Log Start ---\") # Goes to file\n",
    "    train_text_phase(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    pad_token_id=pad_id, \n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "    print(\"--- Training Log End ---\") # Goes to file\n",
    "print(\"Training Finished.\") # Prints to notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT INFERENCE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from text_inference import generate_finding\n",
    "\n",
    "# --- 1. Define Model Structure and Load Trained Weights ---\n",
    "print(\"Initializing model for inference...\")\n",
    "# Ensure these parameters EXACTLY match the trained model\n",
    "model_inf = JointSegTextUNet(\n",
    "    seg_out_channels=1,\n",
    "    vocab_size=30524, \n",
    "    embed_dim=EMBED_DIM,\n",
    "    nhead=NHEAD,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    max_text_seq_len=MAX_TEXT_SEQ_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    pad_token_id=pad_id \n",
    ").to(device)\n",
    "\n",
    "model_path = \"medsegtext_text_phase.pth\"\n",
    "try:\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model_inf.load_state_dict(state_dict)\n",
    "    model_inf.to(device)\n",
    "    print(f\"Model weights loaded successfully from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {e}\")\n",
    "    \n",
    "\n",
    "model_inf.eval()\n",
    "\n",
    "# --- 2. Generate Findings for 5 Test Samples ---\n",
    "print(\"\\n--- Generating Findings for Test Samples ---\")\n",
    "num_samples_to_test = 5\n",
    "samples_tested = 0\n",
    "\n",
    "try:\n",
    "    test_iter = iter(test_loader)\n",
    "\n",
    "    while samples_tested < num_samples_to_test:\n",
    "        print(f\"\\nRequesting batch from test loader (Processed {samples_tested}/{num_samples_to_test})...\")\n",
    "        images_batch, _, _, target_ids_batch, _ = next(test_iter)\n",
    "\n",
    "        for i in range(images_batch.size(0)):\n",
    "            if samples_tested >= num_samples_to_test:\n",
    "                break\n",
    "\n",
    "            print(f\"\\n--- Sample {samples_tested + 1} ---\")\n",
    "\n",
    "            single_image_tensor = images_batch[i].to(device) # Shape: [C, H, W]\n",
    "            ground_truth_ids = target_ids_batch[i].tolist()\n",
    "\n",
    "\n",
    "            try:\n",
    "                # Convert tensor for display: CHW -> HWC\n",
    "                # Assumes tensor is float, range [0, 1] after ToTensorV2\n",
    "                img_display = single_image_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "                # Clamp values just in case they are slightly outside [0, 1] due to transforms\n",
    "                img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "                plt.figure(figsize=(5, 5)) # Adjust size as needed\n",
    "                # Check if image is grayscale (C=1) or color (C=3)\n",
    "                if img_display.shape[2] == 1:\n",
    "                    plt.imshow(img_display.squeeze(), cmap='gray') # Use grayscale colormap\n",
    "                else:\n",
    "                    plt.imshow(img_display)\n",
    "                plt.title(f\"Original Image (Sample {samples_tested + 1})\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            except Exception as plot_e:\n",
    "                print(f\"Error displaying image: {plot_e}\")\n",
    "            \n",
    "\n",
    "            # Decode Ground Truth Text\n",
    "            try:\n",
    "                if pad_id in ground_truth_ids:\n",
    "                     first_pad_index = ground_truth_ids.index(pad_id)\n",
    "                     ground_truth_ids = ground_truth_ids[:first_pad_index]\n",
    "                ground_truth_text = tokenizer.decode(ground_truth_ids, skip_special_tokens=True)\n",
    "                print(f\"Ground Truth: {ground_truth_text}\")\n",
    "            except Exception as decode_e:\n",
    "                 print(f\"Error decoding ground truth: {decode_e}\")\n",
    "                 ground_truth_text = \"[Error decoding]\"\n",
    "\n",
    "            # Generate Predicted Text\n",
    "            try:\n",
    "                generated_finding = generate_finding(\n",
    "                    model=model_inf,\n",
    "                    tokenizer=tokenizer,\n",
    "                    image_tensor=single_image_tensor,\n",
    "                    device=device,\n",
    "                    max_length=MAX_TEXT_SEQ_LEN\n",
    "                )\n",
    "                print(f\"Predicted:    {generated_finding}\")\n",
    "            except Exception as gen_e:\n",
    "                print(f\"Error generating finding: {gen_e}\")\n",
    "                generated_finding = \"[Error generating]\"\n",
    "\n",
    "            samples_tested += 1\n",
    "\n",
    "except StopIteration:\n",
    "    print(f\"\\nTest loader exhausted after processing {samples_tested} samples.\")\n",
    "except NameError as e:\n",
    "    print(f\"\\nA required variable is not defined: {e}\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn error occurred during testing loop: {e}\")\n",
    "     import traceback\n",
    "     traceback.print_exc()\n",
    "\n",
    "print(f\"\\n--- Finished testing {samples_tested} samples ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with metrics\n",
    "from text_inference import evaluate_model # Assuming the function is in text_inference.py\n",
    "\n",
    "# --- Define Model Configuration (parameters used for training) ---\n",
    "model_config = {\n",
    "    'seg_out_channels': 1,\n",
    "    'vocab_size': 30524, # Make sure this is correct\n",
    "    'embed_dim': EMBED_DIM, # Assuming these are defined earlier in notebook\n",
    "    'nhead': NHEAD,\n",
    "    'num_decoder_layers': NUM_DECODER_LAYERS,\n",
    "    'dim_feedforward': DIM_FEEDFORWARD,\n",
    "    'max_text_seq_len': MAX_TEXT_SEQ_LEN,\n",
    "    'dropout': DROPOUT,\n",
    "    'pad_token_id': pad_id # Assuming pad_id is defined earlier\n",
    "}\n",
    "saved_model_path = \"medsegtext_text_phase.pth\"\n",
    "\n",
    "# --- Call the Evaluation Function ---\n",
    "# Ensure test_loader, tokenizer, device are available from previous cells\n",
    "evaluation_results = evaluate_model(\n",
    "    model_path=saved_model_path,\n",
    "    model_config=model_config,\n",
    "    test_loader=test_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    display_limit=5 # How many samples to show images/text for\n",
    ")\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"\\nOverall Evaluation Metrics:\")\n",
    "    for metric, score in evaluation_results.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nEvaluation did not produce results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGEMENTATION TESTING\n",
    "# print(\"Starting Testing...\") # Prints to notebook\n",
    "# with log_stdout_to_file(LOG_FILE_PATH, 'a'): # Use 'a' to append to the log file\n",
    "#     print(\"--- Testing Log Start ---\") # Goes to file\n",
    "\n",
    "#     model.eval()\n",
    "#     total_dice, total_iou, num_samples = 0, 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i, (image, mask, text) in enumerate(test_loader):\n",
    "#             # Model computations on GPU\n",
    "#             image, mask = image.to(device), mask.to(device)\n",
    "#             pred = model(image, text)\n",
    "#             pred = torch.sigmoid(pred)\n",
    "\n",
    "#             # Metrics computation on GPU\n",
    "#             dice = dice_score(pred, mask).item()\n",
    "#             iou = iou_score(pred, mask).item()\n",
    "#             total_dice += dice\n",
    "#             total_iou += iou\n",
    "#             num_samples += 1\n",
    "\n",
    "#             # Visualization - need to move to CPU for matplotlib\n",
    "#             if i < 5:  \n",
    "#                 plt.figure(figsize=(20, 5))\n",
    "                \n",
    "#                 # Move to CPU only for visualization\n",
    "#                 img_display = image.cpu()[0].permute(1, 2, 0).numpy()\n",
    "#                 mask_display = mask.cpu()[0].squeeze().numpy()\n",
    "#                 pred_display = pred.cpu()[0].squeeze().numpy()\n",
    "                \n",
    "#                 plt.subplot(1, 3, 1)\n",
    "#                 plt.imshow(img_display)\n",
    "#                 plt.title(\"Input Image\")\n",
    "                \n",
    "#                 plt.subplot(1, 3, 2)\n",
    "#                 plt.imshow(mask_display, cmap=\"gray\")\n",
    "#                 plt.title(\"Ground Truth\")\n",
    "                \n",
    "#                 plt.subplot(1, 3, 3)\n",
    "#                 plt.imshow(pred_display, cmap=\"gray\")\n",
    "#                 plt.title(f\"Prediction\\nDice: {dice:.4f}, IoU: {iou:.4f}\")\n",
    "                \n",
    "#                 plt.tight_layout()\n",
    "#                 plt.show()\n",
    "#                 plot_path = os.path.join(TEST_OUTPUT_DIR, f\"test_case_{i}.png\")\n",
    "#                 plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "#                 plt.close()\n",
    "        \n",
    "#                 print(f\"Saved test visualization: {plot_path}\")\n",
    "\n",
    "#     print(f\"Test Dice Score: {total_dice / num_samples:.4f}\")\n",
    "#     print(f\"Test IoU Score: {total_iou / num_samples:.4f}\")\n",
    "#     print(\"--- Testing Log End ---\") # Goes to file\n",
    "# print(\"Testing Finished.\") # Prints to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGEMENTATION TESTING\n",
    "# print(\"Starting Testing with threshold output...\") # Prints to notebook\n",
    "# with log_stdout_to_file(LOG_FILE_PATH, 'a'): # Use 'a' to append to the log file\n",
    "#     print(\"--- Testing Log Start ---\") # Goes to file\n",
    "\n",
    "#     model.eval()\n",
    "#     total_dice, total_iou, num_samples = 0, 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i, (image, mask, text) in enumerate(test_loader):\n",
    "#             # Model computations on GPU\n",
    "#             image, mask = image.to(device), mask.to(device)\n",
    "#             pred = model(image, text)\n",
    "#             pred = torch.sigmoid(pred)\n",
    "\n",
    "#             # Metrics computation on GPU\n",
    "#             dice = dice_score(pred, mask).item()\n",
    "#             iou = iou_score(pred, mask).item()\n",
    "#             total_dice += dice\n",
    "#             total_iou += iou\n",
    "#             num_samples += 1\n",
    "\n",
    "#             # Visualization - need to move to CPU for matplotlib\n",
    "#             if i < 5:\n",
    "#                 plt.figure(figsize=(20, 5))\n",
    "\n",
    "#                 # Move to CPU only for visualization\n",
    "#                 img_display = image.cpu()[0].permute(1, 2, 0).numpy()\n",
    "#                 mask_display = mask.cpu()[0].squeeze().numpy()\n",
    "#                 pred_probabilities_display = pred.cpu()[0].squeeze().numpy() # Get probabilities\n",
    "\n",
    "#                 # --- Apply threshold for visualization ---\n",
    "#                 threshold = 0.5\n",
    "#                 pred_binary_display = (pred_probabilities_display > threshold).astype(float) # Convert boolean to float for imshow\n",
    "#                 # -----------------------------------------\n",
    "\n",
    "#                 plt.subplot(1, 3, 1)\n",
    "#                 plt.imshow(img_display)\n",
    "#                 plt.title(\"Input Image\")\n",
    "\n",
    "#                 plt.subplot(1, 3, 2)\n",
    "#                 plt.imshow(mask_display, cmap=\"gray\")\n",
    "#                 plt.title(\"Ground Truth\")\n",
    "\n",
    "#                 plt.subplot(1, 3, 3)\n",
    "#                 # --- Use the binary mask for plotting ---\n",
    "#                 plt.imshow(pred_binary_display, cmap=\"gray\")\n",
    "#                 # ----------------------------------------\n",
    "#                 plt.title(f\"Prediction (Thresholded > {threshold})\\nDice: {dice:.4f}, IoU: {iou:.4f}\")\n",
    "\n",
    "#                 plt.tight_layout()\n",
    "#                 plt.show() \n",
    "#                 plot_path = os.path.join(TEST_OUTPUT_DIR, f\"thresholded_test_case_{i}.png\")\n",
    "#                 plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "#                 plt.close()\n",
    "\n",
    "#                 print(f\"Saved test visualization: {plot_path}\") # This will go to the log file\n",
    "\n",
    "#     # --- Recalculate average metrics AFTER the loop ---\n",
    "#     avg_dice = total_dice / num_samples\n",
    "#     avg_iou = total_iou / num_samples\n",
    "#     # ----------------------------------------------------\n",
    "\n",
    "#     # --- Print final averages to the log file ---\n",
    "#     print(f\"Average Test Dice Score: {avg_dice:.4f}\")\n",
    "#     print(f\"Average Test IoU Score: {avg_iou:.4f}\")\n",
    "#     # ------------------------------------------\n",
    "#     print(\"--- Testing Log End ---\") # Goes to file\n",
    "# print(\"Testing Finished.\") # Prints to notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
